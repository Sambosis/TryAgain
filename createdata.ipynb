{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1_fsvGcXxZkITw56j3kD5UBdFv516L9ar",
      "authorship_tag": "ABX9TyNXGnTR6EnWrpW40bx3ie8E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sambosis/TryAgain/blob/main/createdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7txyu55KW99"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import nltk\n",
        "import warnings\n",
        "import tqdm\n",
        "from spacy import displacy\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from collections import defaultdict\n",
        "\n",
        "def extract_verb_object_pairs(doc):\n",
        "    verb_object_pairs = []\n",
        "    for token in doc:\n",
        "        if \"VERB\" in token.pos_:\n",
        "            # Extract the verb lemma\n",
        "            verb_lemma = token.lemma_\n",
        "            # Find the direct object of the verb\n",
        "            dobj = [child for child in token.children if child.dep_ == 'dobj']\n",
        "            if dobj:  # If a direct object is found\n",
        "                phrase = ''.join(w.text_with_ws for w in dobj[0].subtree).strip()\n",
        "                verb_object_pairs.append((verb_lemma, phrase))\n",
        "    return verb_object_pairs\n",
        "\n",
        "\n",
        "def load_texts_from_directory(directory_path):\n",
        "    all_texts = []\n",
        "    for root, dirs, files in os.walk(directory_path):\n",
        "        for filename in files:\n",
        "            if filename.endswith('.txt'):\n",
        "                with open(os.path.join(root, filename), 'r') as f:\n",
        "                    all_texts.append(f.read())\n",
        "    return all_texts\n",
        "\n",
        "def sentiment_score(text):\n",
        "    sentiment_score = sia.polarity_scores(text)['compound']\n",
        "    return sentiment_score\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_text(raw_text):\n",
        "    # Remove newline characters and replace with spaces\n",
        "    cleaned_text = raw_text.replace('\\n', ' ')\n",
        "\n",
        "    # Use regex to insert spaces between concatenated words\n",
        "    cleaned_text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', cleaned_text)\n",
        "\n",
        "    # Use regex to insert spaces before words that start with a capital letter and follow a lowercase letter\n",
        "    cleaned_text = re.sub(r'([a-z])([A-Z][a-z])', r'\\1 \\2', cleaned_text)\n",
        "    # Various cleaning operations\n",
        "    cleaned_text = re.sub(r'@\\w+', '', cleaned_text)\n",
        "    cleaned_text = re.sub(r'<[^>]+>', '', cleaned_text)\n",
        "    cleaned_text = re.sub(r'@\\s', '', cleaned_text)\n",
        "    cleaned_text = re.sub(r'\\\\', '', cleaned_text)\n",
        "    cleaned_text = re.sub(r'/', '', cleaned_text)\n",
        "\n",
        "    # Handle contractions\n",
        "    contractions = {\n",
        "        r\" n\\\\'t\": \"n't\",\n",
        "        r\" \\\\'re\": \"'re\",\n",
        "        r\" \\\\'s\": \"'s\",\n",
        "        r\" \\\\'d\": \"'d\",\n",
        "        r\" \\\\'ll\": \"'ll\",\n",
        "        r\" \\\\'t\": \"'t\",\n",
        "        r\" \\\\'ve\": \"'ve\",\n",
        "        r\" \\\\'m\": \"'m\"\n",
        "    }\n",
        "\n",
        "    for contraction, replacement in contractions.items():\n",
        "        cleaned_text = re.sub(contraction, replacement, cleaned_text)\n",
        "    cleaned_text = re.sub(r'\\d', '', cleaned_text)\n",
        "    cleaned_text = cleaned_text.replace('\\r\\n', ' ')\n",
        "    return cleaned_text\n",
        "\n",
        "def extract_verb_phrases_spacy(doc):\n",
        "    verb_phrases = []\n",
        "    for token in doc:\n",
        "        # Check if the token is a verb\n",
        "        if \"VERB\" in token.pos_:\n",
        "            # Extract the verb itself\n",
        "            phrase = token.text_with_ws\n",
        "\n",
        "            # Find the direct object of the verb\n",
        "            dobj = [child for child in token.children if child.dep_ == 'dobj']\n",
        "            if dobj:  # If a direct object is found\n",
        "                phrase += ''.join(w.text_with_ws for w in dobj[0].subtree)\n",
        "                verb_phrases.append(phrase.strip())\n",
        "    return verb_phrases\n",
        "\n",
        "# For a given direct object, identify the most frequently associated verbs\n",
        "def get_associated_verbs(direct_object, grouped_data):\n",
        "    return [verb for verb, objs in grouped_data.items() if direct_object in objs]\n",
        "\n",
        "def extract_and_group_verb_phrases_spacy(doc):\n",
        "    verb_phrases = defaultdict(list)\n",
        "    for token in doc:\n",
        "        # Check if the token is a verb\n",
        "        if \"VERB\" in token.pos_:\n",
        "            # Extract the verb lemma\n",
        "            verb_lemma = token.lemma_\n",
        "            # Find the direct object of the verb\n",
        "            dobj = [child for child in token.children if child.dep_ == 'dobj']\n",
        "            if dobj:  # If a direct object is found\n",
        "                phrase = ''.join(w.text_with_ws for w in dobj[0].subtree)\n",
        "                verb_phrases[verb_lemma].append(phrase.strip())\n",
        "    return verb_phrases\n",
        "\n",
        "# This function splits the text into chunks that are smaller than a specified max length.\n",
        "def split_text_into_chunks(text, max_length):\n",
        "    words = text.split(' ')\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "    for word in words:\n",
        "        if current_length + len(word) < max_length:\n",
        "            current_chunk.append(word)\n",
        "            current_length += len(word) + 1\n",
        "        else:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [word]\n",
        "            current_length = len(word) + 1\n",
        "    chunks.append(' '.join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "def process_chunk_and_extract_data(chunk):\n",
        "    # Process the chunk with spaCy\n",
        "    doc = nlp(chunk)\n",
        "\n",
        "    # Extract verb-object pairs from the chunk\n",
        "    verb_obj_pairs = extract_verb_object_pairs(doc)\n",
        "\n",
        "    # Extract sentences containing the verb-object pairs\n",
        "    sentences_with_vo = extract_sentences_with_verb_object_pairs(doc, verb_obj_pairs)\n",
        "\n",
        "    # Convert the extracted data to a DataFrame\n",
        "    df_chunk = pd.DataFrame(sentences_with_vo)\n",
        "\n",
        "    return df_chunk\n",
        "\n",
        "\n",
        "def extract_sentences_with_verb_object_pairs(doc, verb_obj_pairs):\n",
        "    sentences_with_vo = []\n",
        "    sents_list = list(doc.sents)  # Convert sentences to a list for easier indexing\n",
        "\n",
        "    for idx, sent in enumerate(sents_list):\n",
        "        for verb, obj in verb_obj_pairs:\n",
        "            if verb in sent.text and obj in sent.text:\n",
        "                # Extract preceding, action and following sentences\n",
        "                preceding_sent = sents_list[idx-1].text if idx > 0 else ''\n",
        "                action_sent = sent.text\n",
        "                following_sent = sents_list[idx+1].text if idx < len(sents_list)-1 else ''\n",
        "\n",
        "                # Calculate sentiment scores\n",
        "                pre_sentiment = sentiment_score(preceding_sent)\n",
        "                action_sentiment = sentiment_score(action_sent)\n",
        "                post_sentiment = sentiment_score(following_sent)\n",
        "\n",
        "                sentences_with_vo.append({\n",
        "                    \"Preceding Sentence\": preceding_sent.strip(),\n",
        "                    \"Action Sentence\": action_sent.strip(),\n",
        "                    \"Following Sentence\": following_sent.strip(),\n",
        "                    \"Verb\": verb,\n",
        "                    \"Object\": obj,\n",
        "                    \"Pre-Sentiment Score\": pre_sentiment,\n",
        "                    \"Action Sentiment Score\": action_sentiment,\n",
        "                    \"Post-Sentiment Score\": post_sentiment\n",
        "                })\n",
        "\n",
        "    return sentences_with_vo\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "texts = load_texts_from_directory('/content/drive/MyDrive/data')\n",
        "raw_text = ''\n",
        "with open(\"cleantext.txt\", \"w\") as file:\n",
        "  for text in texts:\n",
        "    raw_text 3+= text\n"
      ],
      "metadata": {
        "id": "1qtbNaDgVUUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = preprocess_text(raw_text)"
      ],
      "metadata": {
        "id": "Uz2j76bhNxl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PKfx1VAXYE6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 1000000  # Adjust this based on your needs\n",
        "chunks = split_text_into_chunks(cleaned_text, MAX_LENGTH)"
      ],
      "metadata": {
        "id": "VbHEu0AvYELQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_dataframes = []\n",
        "\n",
        "for chunk in chunks:\n",
        "    df_chunk = process_chunk_and_extract_data(chunk)\n",
        "    all_dataframes.append(df_chunk)\n",
        "\n",
        "full_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "full_df.to_csv(\"action_states.csv\")\n",
        "full_df"
      ],
      "metadata": {
        "id": "he9f6epiZy1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_df.to_csv(\"/content/drive/MyDrive/action_states.csv\")"
      ],
      "metadata": {
        "id": "gnNoI8-d-C1Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}